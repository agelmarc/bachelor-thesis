Before we introduce the complete workflow to train a network on NCSM ground state energy sequences, we first have to decide on the network structure to use.
This includes the structure of the input layer, the structure of the output layer, as well as the structure of the hidden layers.

Since NCSM calculations are usually done with multiple different oscillator frequencies $\hbar \Omega$, and different oscillator frequencies result in different convergence rates of the energy sequences, we want to incorporate more then one frequency into the extrapolation in order to give the maximum amount of information about the interaction and the nucleus into the network.
For that, we do not only try to extrapolate a single energy sequence, but input multiple sequences for different oscillator frequencies into the network.
We decide on 3 energy sequences with given frequencies to input into the network, since that is the amount of frequencies which are usually available from NCSM calculations of higher-mass nuclei.

In order to determine the structure of the input layer, we must further choose a fixed length of the energy sequences which are put into the network.
For NCSM calculations for nuclei of higher mass, there are not many $N_\mathrm{max}$ values available, which means that we have to intentionally keep the sequence langth small enough.
We decide on energy sequences of length 4.
In summary, the input layer thus has to consist of $L^{(1)} = 3\times 4 = 12$ input neurons.

Since the output of the neural network should be a single real number indicating the extrapolated ground state energy, the output layer consists of $L^{(N)} = 1$ neuron.

For the hidden layers, there is no generally accepted method of defining the most efficient structure. For the most part, the count of the hidden layers as well as the count of the neurons in those layers have to be chosen empirically, such that the network neither underfits nor overfits the data. For our framework, we choose $N = 4$ total layers, such that there are two hidden layers of $L^{(2)} = 24$ and $L^{(3)} = 12$ neurons.

% Activation Function
Each layer besides the output layer will be given a \textit{rectifier} activation function given by
\begin{equation}
  \sigma(z) = \max(0, z),
\end{equation}
which restricts the activation of the neurons to be positive. This function is chosen because it empirically yields the best results for the extrapolation. Notice that the activations of the last layer will not get rectified, as the extrapolations, and therefore the network output, are not limited to positive ground state energies. As a consequence, the connections of the second-last layer to the last layer will also be responsible for the sign of the extrapolated energy.

% Loss function
The loss function of our network is a standard \textit{Mean Squared Error} (MSE) loss given for a single network input $x$ by
\begin{equation}
  C_x = \norm{f(x)-o}^2,
\end{equation}
where $f(x)$ is the vector of activations of the neurons in the output layer given the input of $x$, and $o$ is the vector of desired activations. For $n$ input samples $x_i$ with desired outputs $o_i$, the loss is averaged over all samples yielding
\begin{equation}
  C = \frac{1}{n}\sum_{i=1}^n C_{x_i}=\frac{1}{n} \sum_{i=1}^{n}\norm{f(x_i)-o_i}^2
\end{equation}

% Optimization algorithm
For the optimization process, we opt for the \textit{AdamW} algorithm. This is a variant \cite{adamw} of the \textit{Adam} algorithm, which is based on the simple stochastic gradient descent algorithm introduced in \autoref{sec:sgd}.

% Implementation
To implement the neural network, we use \textit{pytorch}, which is a machine learning library for Python.

\subsection{Input data}
Since the network has to be trained with
