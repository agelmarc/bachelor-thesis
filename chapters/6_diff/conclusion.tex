In this chapter, we introduced a second framework for extrapolating ground-state energies using neural networks. Here, we do not directly input the absolute energies into the network in return of an absolute energy prediction, but calculate the differences of consecutive energies in our sequences to input to the network. The output of the network was then trained on a prediction target calculated using the average distance of the last values in the formatted sequences to the actual limit. As a consequence, the evaluation process had to reconstruct the absolute energy prediction by adding the average energy for the highest $N_\mathrm{max}$ back onto the networks prediction of the difference.

In order to further examine the training modifications done in the previous chapter, we have evaluated our three training nuclei \n{2}{H}, \n{3}{H} and \n{4}{He} using the additional training modifications.

We have found that the difference based extrapolation framework generally produced predictions with a smaller uncertainty. Further, the extrapolations generated were more reasonable than those generated by our previous absolute based framework, as they were generally too low for small $N_\mathrm{max}$ values.

As for the training modes, we have found that they do not impact the absolute value of the prediction as much as in the absolute based framework. Here, they have impacted the uncertainties of the predictions, where the SRG-filter training mode generally impacted the improvement of the uncertainties the most.
Before we reach a final decision on the two different frameworks and the three training modes, we will see how they affect the extrapolation of the nucleus \n{6}{Li} in the next chapter, which will be more close to a real use-case of extrapolation methods.
