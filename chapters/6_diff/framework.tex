In order to implement our difference based extrapolation framework, we have to think about how we have to change the generation of the input data, the network structure as well as the extrapolation process of our basic extrapolation framework.

We want to base our network training on the same data set as the basic framework. The networks now should get the differences of consecutive ground-state energies in the sequences as an input. To achieve this, we generate the same formatted input sequences out of the raw NCSM calculations, using the inflation algorithm described in \autoref{sec:inflate}. The formatted input data thus consists of four consecutive energy values for three oscillator frequencies, as well as a limit which will be used to compute the prediction target of our networks. To turn those sequences into our input difference sequences, we calculate the three differences between the four energy values. When the networks get input data in the form of energy differences, they cannot possibly predict an absolute ground-state energy from them. This means that the prediction target will also have to be modified to a difference. It is natural to take the difference between the last value of the energy sequence and the limit as a target. However, this leads to a problem, as the limit is the same for the three oscillator frequencies for a given nucleus and interaction, but the last values of the energy sequences can vary depending on the frequency. Because the three difference sequences are all put into a network simultaneously, we have to find a single common extrapolation target. For this, we decide to take the difference between the mean of the last values for all three sequences and the limit.

Note that, since we now only have three direct inputs available, which are the differences of the four absolute values, the network structure has to be adjusted to be compatible with this data. For the input layer, we have $L^{(1)} = 3 \times 3 = 9$ input neurons. Furthermore, we reduce some of the hidden neurons in the hidden layers to accommodate the reduced input neurons. We empirically decide on $L^{(2)} = 18$, $L^{(3)} = 12$. The single output neuron stays the same. We could have chosen to keep the network structure the same, but this would require to input four differences of five energies, which would mean that those networks got more information about the sequences. To ensure comparability, we chose to keep the information the same but adjust the network structure.

Further modifications include the adjustment of our sequence shifting. In our basic framework, we decided to shift the formatted sequences by a random amount of $[\SI{-10}{\mega\electronvolt}, \SI{10}{\mega\electronvolt}]$ to counter the dependency on absolute values. They are no longer needed, as they would cancel out when calculating the differences. Also, the threshold for deeming a network valid is reduced to \SI{0.1}{\mega\electronvolt}, as the energy differences which are input are generally much smaller than the absolute values.

To compensate for the shift in the training set, we also have to modify the evaluation of a network. The evaluation will work the same way as in our basic framework, but now, the final prediction is calculated by adding the mean of the last energy values back onto the network prediction.
